{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfnwB4IIfq4uN8+9Wxx6Co",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daikumatan/faiss-handson/blob/main/HandsOn_multilingual_e5_on_FAISS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAISS Installation"
      ],
      "metadata": {
        "id": "zAwAf1Ieo8Uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UTzR-pynpbb",
        "outputId": "4a578618-6439-4105-ac85-b45a4656bb5c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding"
      ],
      "metadata": {
        "id": "j74atEVio_cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### embedding for VectorDB"
      ],
      "metadata": {
        "id": "3vO62FP6MAme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリをインポートします。\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "import faiss  # FAISSライブラリをインポート\n",
        "import numpy as np  # NumPyライブラリをインポート\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# 最後の隠れ層の状態を平均プーリングする関数を定義します。\n",
        "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
        "    # print(attention_mask) # 情報があるところが1, paddingが0で表示\n",
        "    # print(attention_mask[..., None]) # 次元を一つ追加して3次元にしている\n",
        "    # print(attention_mask[..., None].bool()) # 1をTrue, 0をFalseへ変換\n",
        "    # print(~attention_mask[..., None].bool()) # TrueとFalseを入れ替え (paddingがTrueになる)\n",
        "\n",
        "    # last_hidden_statesの値におけるpaddingの値が0.0に置き換えられる\n",
        "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "\n",
        "    # last_hidden.sum(dim=1)でシーケンシャル方向に和を取る\n",
        "    # つまり、[<batch_size>, <sequence_length>, <隠れ層の次元数>] => [<batch_size>, <隠れ層の次元数>] に変換\n",
        "    # attention_mask.sum(dim=1)[..., None]は、padding除いたToken数の和。この値で割って平均化する\n",
        "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "\n",
        "# 解析したいテキストのリストを定義します。\n",
        "input_texts = [\n",
        "    \"好きな食べ物は何ですか?\",\n",
        "    \"どこにお住まいですか?\",\n",
        "    \"朝の電車は混みますね\",\n",
        "    \"今日は良いお天気ですね\",\n",
        "    \"最近景気悪いですね\",\n",
        "    #\"最近、出かけていないので、たまには外で食事でもどうですか？\"\n",
        "]\n",
        "\n",
        "# 使用するモデルとトークナイザーの事前学習済みのパスを指定してロードします。\n",
        "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')\n",
        "model = AutoModel.from_pretrained('intfloat/multilingual-e5-large')\n",
        "\n",
        "# テキストをモデルが処理できる形式にトークン化します。\n",
        "batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# batch_dict内の各テンソルのサイズを確認する\n",
        "for i, (key, tensor) in enumerate(batch_dict.items()):\n",
        "    print(f\"[{i}][batch_dict] {key}: {tensor.size()}\")\n",
        "\n",
        "# モデルを実行して出力を取得します。\n",
        "outputs = model(**batch_dict)\n",
        "\n",
        "# batch_dict内の各テンソルのサイズを確認する\n",
        "for i, (key, tensor) in enumerate(outputs.items()):\n",
        "    print(f\"[{i}][outputs] {key}: {tensor.size()}\")\n",
        "\n",
        "# `average_pool`関数を使って、最終的な埋め込みベクトルを計算します。\n",
        "# print(\"attention_mask:\", batch_dict['attention_mask'].sum(dim=1))\n",
        "# print(\"outputs.last_hidden_state.sum(dim=1).shape:\", outputs.last_hidden_state.sum(dim=1).shape)\n",
        "# print(\"last_hidden_states:\", outputs.last_hidden_state.sum(dim=1))\n",
        "embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
        "\n",
        "# PyTorchのTensorからNumPy配列に変換します。\n",
        "embeddings_np = embeddings.cpu().detach().numpy()\n",
        "print(\"embeddings_np:\", embeddings_np.shape)\n",
        "\n",
        "# わかりやすさのための表示\n",
        "for i, vec in enumerate(embeddings_np[:]):\n",
        "    print(f\"----{i}----\")\n",
        "    print(input_texts[i])\n",
        "    print(vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ArvWlRSmOP3",
        "outputId": "bff43cd4-7b94-4364-e254-f0607e474eb9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0][batch_dict] input_ids: torch.Size([5, 10])\n",
            "[1][batch_dict] attention_mask: torch.Size([5, 10])\n",
            "[0][outputs] last_hidden_state: torch.Size([5, 10, 1024])\n",
            "[1][outputs] pooler_output: torch.Size([5, 1024])\n",
            "embeddings_np: (5, 1024)\n",
            "----0----\n",
            "好きな食べ物は何ですか?\n",
            "[ 0.6108155  -0.2260578  -0.10909441 ... -0.62022376 -0.83260953\n",
            "  0.28850102]\n",
            "----1----\n",
            "どこにお住まいですか?\n",
            "[ 0.99075234  0.6782665  -0.68046105 ... -0.7541518  -1.3915895\n",
            " -0.14331307]\n",
            "----2----\n",
            "朝の電車は混みますね\n",
            "[ 1.1466048   0.16233358 -0.02154386 ...  0.07610887 -0.13563985\n",
            "  0.2248166 ]\n",
            "----3----\n",
            "今日は良いお天気ですね\n",
            "[ 0.9829717   0.06835324 -0.34694287 ...  0.00633434 -0.7837953\n",
            " -0.28379193]\n",
            "----4----\n",
            "最近景気悪いですね\n",
            "[ 0.26057956  0.5884724  -0.49848753 ... -0.19543631 -0.8918747\n",
            " -1.4221884 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# トークン化された結果を表示します。\n",
        "tokenized_input = tokenizer.batch_encode_plus(input_texts,\n",
        "                                              max_length=512,\n",
        "                                              padding=True,\n",
        "                                              truncation=True,\n",
        "                                              return_tensors='pt')\n",
        "#print(tokenized_input)\n",
        "\n",
        "# トークンIDからトークン文字列への変換と、[UNK]のチェック\n",
        "decoded_tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in tokenized_input['input_ids']]\n",
        "unk_tokens = [[token for token in tokens if token == tokenizer.unk_token] for tokens in decoded_tokens]\n",
        "\n",
        "# それぞれの入力テキストと対応するトークン、[UNK]トークンの有無を表示\n",
        "for text, tokens, unk in zip(input_texts, decoded_tokens, unk_tokens):\n",
        "    print(f\"Input Text: {text}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"UNK Tokens: {unk}\")\n",
        "    print(\"----------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUw7v6x76ekn",
        "outputId": "ff873c7c-ac07-4dce-fb91-1ca134df8ebf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text: 好きな食べ物は何ですか?\n",
            "Tokens: ['<s>', '▁', '好きな', '食べ物', 'は何', 'ですか', '?', '</s>', '<pad>', '<pad>']\n",
            "UNK Tokens: []\n",
            "----------\n",
            "Input Text: どこにお住まいですか?\n",
            "Tokens: ['<s>', '▁', 'どこに', 'お', '住', 'まい', 'ですか', '?', '</s>', '<pad>']\n",
            "UNK Tokens: []\n",
            "----------\n",
            "Input Text: 朝の電車は混みますね\n",
            "Tokens: ['<s>', '▁', '朝', 'の', '電車', 'は', '混', 'みます', 'ね', '</s>']\n",
            "UNK Tokens: []\n",
            "----------\n",
            "Input Text: 今日は良いお天気ですね\n",
            "Tokens: ['<s>', '▁今日は', '良い', 'お', '天気', 'ですね', '</s>', '<pad>', '<pad>', '<pad>']\n",
            "UNK Tokens: []\n",
            "----------\n",
            "Input Text: 最近景気悪いですね\n",
            "Tokens: ['<s>', '▁最近', '景', '気', '悪い', 'ですね', '</s>', '<pad>', '<pad>', '<pad>']\n",
            "UNK Tokens: []\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### query text embedding"
      ],
      "metadata": {
        "id": "lsO2-xSwL81W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# クエリテキストを定義します。\n",
        "query_text = [\"今日は雨が振らなくてよかった\"]\n",
        "\n",
        "# クエリテキストをトークナイズし、モデルが処理できる形式にします。\n",
        "query_dict = tokenizer(query_text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# モデルを実行して出力を取得します。\n",
        "query_output = model(**query_dict)\n",
        "\n",
        "# `average_pool`関数を使って、クエリの埋め込みベクトルを計算します。\n",
        "query_embeddings = average_pool(query_output.last_hidden_state, query_dict['attention_mask'])\n",
        "\n",
        "# クエリの埋め込みベクトルをNumPy配列に変換します。\n",
        "query_embeddings_np = query_embeddings.cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "4p0ZTi4GoSG3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAISS setup\n"
      ],
      "metadata": {
        "id": "TfEFOYF2pcaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- IndexFlatL2\n",
        "    - ベクトル間のユークリッド距離（L2距離）を使用して類似性を計測します。全データ点に対して線形探索を行うため、計算は正確ですが、大規模データセットでは計算コストが高くなります。\n",
        "- IndexFlatIP\n",
        "    - いわゆるcos類似度でありベクトル間の内積を用いて類似性を評価します。この指標は類似度が高いほど大きな値となるため、類似性の高いベクトルを効果的に特定できますが、大きなデータセットには適していません。\n",
        "- IndexIVFFlat\n",
        "    - 先にデータをクラスタリングして索引を作成し、検索時には近似的な検索を行います。これにより、IndexFlatL2やIndexFlatIPと比較して高速な検索が可能ですが、精度は若干落ちます。\n",
        "- IndexHNSW\n",
        "    - 階層的ナビゲータブル小世界（Hierarchical Navigable Small World）グラフを使用し、近似最近傍探索を非常に高速に行います。大規模なデータセットに適しており、距離計算の回数を劇的に削減することができます。"
      ],
      "metadata": {
        "id": "PmdEEitkPk2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# ベクトルの次元数を設定\n",
        "dimension = embeddings_np.shape[1]\n",
        "\n",
        "# IndexFlatL2インデックスの初期化とデータ追加\n",
        "index_flat_l2 = faiss.IndexFlatL2(dimension)\n",
        "index_flat_l2.add(embeddings_np)\n",
        "print(\"Number of vectors in the IndexFlatL2:\", index_flat_l2.ntotal)\n",
        "\n",
        "# IndexFlatIP（内積を使用するインデックス）の初期化とデータ追加\n",
        "index_flat_ip = faiss.IndexFlatIP(dimension)\n",
        "index_flat_ip.add(embeddings_np)\n",
        "print(\"Number of vectors in the IndexFlatIP:\", index_flat_ip.ntotal)\n",
        "\n",
        "# IndexIVFFlat（量子化されたインデックス）の初期化\n",
        "# クラスタの数を設定。通常はデータの量によって決定されます。\n",
        "nlist = 4  # クラスタ数\n",
        "quantizer = faiss.IndexFlatL2(dimension)  # 量子化器としてIndexFlatL2を使用\n",
        "index_ivf_flat = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_L2)\n",
        "index_ivf_flat.train(embeddings_np)\n",
        "index_ivf_flat.add(embeddings_np)\n",
        "print(\"Number of vectors in the IndexIVFFlat:\", index_ivf_flat.ntotal)\n",
        "\n",
        "# IndexHNSW（階層的ナビゲーティブ小世界グラフを使用するインデックス）の初期化とデータ追加\n",
        "index_hnsw = faiss.IndexHNSWFlat(dimension, 10)  # ここで10はHNSWのグラフの近隣接続数\n",
        "index_hnsw.add(embeddings_np)\n",
        "print(\"Number of vectors in the IndexHNSW:\", index_hnsw.ntotal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN4TlykDLSR8",
        "outputId": "294453bd-e059-4102-dca3-ca77d8e16bff"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of vectors in the IndexFlatL2: 5\n",
            "Number of vectors in the IndexFlatIP: 5\n",
            "Number of vectors in the IndexIVFFlat: 5\n",
            "Number of vectors in the IndexHNSW: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Queryの実行"
      ],
      "metadata": {
        "id": "rvW6i0JTPJkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 上位3つの近いベクトルを検索するための設定（k=3）\n",
        "k = 2\n",
        "\n",
        "# クエリの処理（以前のコードからのクエリテキストの部分を続ける）\n",
        "query_embeddings_np = query_embeddings.cpu().detach().numpy()\n",
        "\n",
        "# 各インデックスに対して検索を実行し、結果を取得\n",
        "D_l2, I_l2 = index_flat_l2.search(query_embeddings_np, k)\n",
        "D_ip, I_ip = index_flat_ip.search(query_embeddings_np, k)\n",
        "D_ivf, I_ivf = index_ivf_flat.search(query_embeddings_np, k)\n",
        "D_hnsw, I_hnsw = index_hnsw.search(query_embeddings_np, k)\n",
        "\n",
        "# 結果を表示する関数\n",
        "def display_results(D, I, input_texts, index_type):\n",
        "    print(f\"Results for {index_type}:\")\n",
        "    for i in range(k):\n",
        "        print(f\"  Rank: {i+1}, Index: {I[0][i]}, Distance: {D[0][i]}, Text: '{input_texts[I[0][i]]}\")\n",
        "\n",
        "# 各インデックスタイプの結果を表示\n",
        "display_results(D_l2, I_l2, input_texts, \"IndexFlatL2\")\n",
        "display_results(D_ip, I_ip, input_texts, \"IndexFlatIP\")\n",
        "display_results(D_ivf, I_ivf, input_texts, \"IndexIVFFlat\")\n",
        "display_results(D_hnsw, I_hnsw, input_texts, \"IndexHNSW\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Tgetj9_NVhI",
        "outputId": "8ef0acb4-f486-4f37-8fbc-d01d9ff245a5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for IndexFlatL2:\n",
            "  Rank: 1, Index: 3, Distance: 202.58700561523438, Text: '今日は良いお天気ですね\n",
            "  Rank: 2, Index: 4, Distance: 267.9387512207031, Text: '最近景気悪いですね\n",
            "Results for IndexFlatIP:\n",
            "  Rank: 1, Index: 3, Distance: 761.3016967773438, Text: '今日は良いお天気ですね\n",
            "  Rank: 2, Index: 4, Distance: 718.5281982421875, Text: '最近景気悪いですね\n",
            "Results for IndexIVFFlat:\n",
            "  Rank: 1, Index: 3, Distance: 202.58700561523438, Text: '今日は良いお天気ですね\n",
            "  Rank: 2, Index: -1, Distance: 3.4028234663852886e+38, Text: '最近景気悪いですね\n",
            "Results for IndexHNSW:\n",
            "  Rank: 1, Index: 3, Distance: 202.58700561523438, Text: '今日は良いお天気ですね\n",
            "  Rank: 2, Index: 4, Distance: 267.9387512207031, Text: '最近景気悪いですね\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 解説"
      ],
      "metadata": {
        "id": "BmKCDEg2s_JG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1rbg_eH8tFVG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}